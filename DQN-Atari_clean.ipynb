{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN-Atari_clean.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOxBRF4Hqu4nlcWz4ctT/jL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Install Libraries and Dependencies"],"metadata":{"id":"Y3abUQ9LxNAh"}},{"cell_type":"markdown","source":["#### Virtual Display"],"metadata":{"id":"VAXDcw60xxAJ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"gHAUDJCzxCWT","executionInfo":{"status":"ok","timestamp":1658567917314,"user_tz":-60,"elapsed":29630,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"outputs":[],"source":["%%capture\n","!apt install python-opengl # Python binding to OpenGL and related APIs\n","!pip install pyglet==1.5.1 \n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay\n","\n","# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"markdown","source":["#### Gym [Atari]"],"metadata":{"id":"xtIQcKTvx3EB"}},{"cell_type":"code","source":["%%capture\n","!pip install gym[atari,accept-rom-license]==0.21.0"],"metadata":{"id":"BUQY8UwvxnhO","executionInfo":{"status":"ok","timestamp":1658567938341,"user_tz":-60,"elapsed":21053,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"HhFdxZZ0yGbA"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import cv2\n","\n","import gym"],"metadata":{"id":"sqFADgdPyAte","executionInfo":{"status":"ok","timestamp":1658567941740,"user_tz":-60,"elapsed":3417,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Gym Environment"],"metadata":{"id":"rFmt_vWbyKi0"}},{"cell_type":"code","source":["ENV_NAME = \"BreakoutNoFrameskip-v4\"\n","env = gym.make(ENV_NAME)"],"metadata":{"id":"LFbiWOD_yJ7O","executionInfo":{"status":"ok","timestamp":1658567942354,"user_tz":-60,"elapsed":625,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["OBSERVATION_SHAPE = env.observation_space.shape\n","ACTION_SPACE_SIZE = env.action_space.n"],"metadata":{"id":"9BG5wjOjyZH0","executionInfo":{"status":"ok","timestamp":1658567942354,"user_tz":-60,"elapsed":22,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#### Environment Wrappers"],"metadata":{"id":"K-ENUhB_ykjG"}},{"cell_type":"markdown","source":["Wrappers doing the following will be implemented:\n","- Downscalling the observation\n","- Grayscaling the observation\n","- Preprocessing the observation to `n` sequence of past frames\n","- Perform `no-op` operations at the start of an episode\n","- Rescaling the reward to discrete values -1, 0, 1\n","- Perform Frame Skipping\n","- Convert the observations to PyTorch Image shapes and `torch.Tensor` object\n","- Make atari end-of-life the end-of-episode, and reset only on game-over.\n"],"metadata":{"id":"dw9jvg8CywBz"}},{"cell_type":"code","source":["from collections import deque\n","\n","cv2.ocl.setUseOpenCL(False) # function\n","\n","# Ref: Extended Data Table 1 | List of hyperparameters and their values\n","class NoOpOnReset(gym.Wrapper):\n","  def __init__(self, env, noop_max=30, noop_action=0):\n","    \"\"\"Applies `noop_action` for a random number of steps in the range \n","    `[1, noop_max]` at the start of an episode\"\"\"\n","    super().__init__(env)\n","    \n","    self.noop_max = noop_max\n","    self.noop_action = noop_action\n","\n","    assert env.unwrapped.get_action_meanings()[noop_action] == \"NOOP\", \"Action meaning for noop_action doesn't match 'NOOP'\"\n","    assert noop_max >= 1, \"noop_max must be >= 1\"\n","\n","  def reset(self, **kwargs):\n","    \"\"\"Do no-op action for a random number of steps in [1, noop_max]\"\"\"\n","    num_noop_actions = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n","\n","    obs = self.env.reset(**kwargs)\n","\n","    for _ in range(num_noop_actions):\n","      obs, _, done, _ = self.env.step(self.noop_action)\n","\n","      if done:\n","        obs = self.env.reset(**kwargs)\n","\n","    return obs\n","\n","  def step(self, action):\n","    \"\"\"Perform a single `action` in the environment\"\"\"\n","    return self.env.step(action)  # needed?"],"metadata":{"id":"PyJhRFvoyjll","executionInfo":{"status":"ok","timestamp":1658567942355,"user_tz":-60,"elapsed":22,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class FireOnReset(gym.Wrapper):\n","  def __init__(self, env):\n","    \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n","    super().__init__(env)\n","    \n","    assert env.unwrapped.get_action_meanings()[1] == \"FIRE\", \"Action 1 should be 'FIRE'\"\n","    \n","    # there should be at least one more action apart from NOOP and FIRE\n","    assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","  def reset(self, **kwargs):\n","    self.env.reset(**kwargs)\n","\n","    obs, _, done, info =  self.env.step(1)\n","    if done:\n","      self.env.reset(**kwargs)\n","    \n","    # why take action 2? (this follows from openai baselines implementation)\n","    obs, _, done, info = self.env.step(2)\n","    if done:\n","      self.env.reset(**kwargs)\n","\n","    return obs\n","\n","  def step(self, act):\n","    return self.env.step(act)"],"metadata":{"id":"IZIPjyuamvRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EpisodicLifeEnv(gym.Wrapper):\n","  def __init__(self, env):\n","    \"\"\"Make end-of-life == end-of-episode, but only resets on true game over\"\"\"\n","    super().__init__(env)\n","\n","    # number of lives the agent has left\n","    self.lives = env.unwrapped.ale.lives()\n","    self.real_done = False  # true game over\n","\n","    assert env.unwrapped.get_action_meanings()[0] == \"NOOP\", \"Meaning of action 0 is not NOOP\"\n","\n","  def step(self, act):\n","    obs, rew, done, info = self.env.step(act)\n","    \n","    # if env returns done=True, then it's game over\n","    self.real_done = done\n","\n","    # get how many the agent have left\n","    lives = self.env.unwrapped.ale.lives()\n","\n","    # if there is a reduction in `lives` compared with previous value stored\n","    # in `self.lives`, that's an end-of-life transition\n","    if lives < self.lives and lives > 0:\n","      # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n","      # so its important to keep lives > 0\n","      done = True\n","\n","    # update the number of lives the agent has left\n","    self.lives = lives\n","    return obs, rew, done, info\n","\n","  def reset(self, **kwargs):\n","    \"\"\"Reset only when lives are exhausted.\n","      This way all states are still reachable even though lives are episodic,\n","      and the learner need not know about any of this behind-the-scenes.\n","    \"\"\"\n","    if self.real_done:\n","      obs = self.env.reset(**kwargs)\n","    else:\n","      # take no-op action to advance from lost life state\n","      obs, _, _, _ = self.env.step(0)\n","\n","    # update the number of lives the agent has left\n","    # in-case the no-op action led to end-of-life\n","    self.lives = self.env.unwrapped.ale.lives()\n","    return obs\n","  "],"metadata":{"id":"0zRwFQ440ecx","executionInfo":{"status":"ok","timestamp":1658567942355,"user_tz":-60,"elapsed":19,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class FrameSkip(gym.Wrapper): # MaxAndSkipEnv\n","  def __init__(self, env, skip=4):\n","    \"\"\"Return only every `skip`-th frame\"\"\"\n","    super().__init__(env)\n","    self.skip = skip\n","    self.obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n","\n","    assert skip >= 1, \"Number of frames to skip should be at least 1\"\n","\n","  def step(self, act):\n","    \"\"\"Repeat action for `skip` frames, sum reward, and max over last observations.\"\"\"\n","    total_reward = 0.0\n","    for i in range(self.skip):\n","      obs, rew, done, info = self.env.step(act)\n","      if i == self.skip - 2: self.obs_buffer[0] = obs\n","      if i == self.skip - 1: self.obs_buffer[1] = obs\n","\n","      total_reward += rew\n","\n","      if done:\n","        # note that the observervation after done does not matter\n","        # so it doesn't matter if we have refilled the obs_buffer or not\n","        break\n","\n","    max_frame = self.obs_buffer.max(axis=0)\n","    return max_frame, total_reward, done, info\n","\n","  def reset(self, **kwargs):\n","    return self.env.reset(**kwargs)\n","\n","  def reset(self):\n","    return self.env.reset()\n"],"metadata":{"id":"pA_Dvvdt-E_c","executionInfo":{"status":"ok","timestamp":1658567942356,"user_tz":-60,"elapsed":19,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class ClipReward(gym.Wrapper):\n","  def __init__(self, env):\n","    super().__init__(env)\n","\n","  def reward(self, rew):\n","    \"\"\"\n","    Bin reward to {+1, 0, -1} by its sign.\n","    Returns -1 if rew < 0, 0 if rew == 0, +1 if rew > 0\n","    \"\"\"\n","    return np.sign(rew)"],"metadata":{"id":"m3mOBwkf-pFg","executionInfo":{"status":"ok","timestamp":1658567942356,"user_tz":-60,"elapsed":18,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class DownscaleFrame(gym.Wrapper):\n","  pass"],"metadata":{"id":"f3VkVkBX_EiS","executionInfo":{"status":"ok","timestamp":1658567942357,"user_tz":-60,"elapsed":17,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class StackFrame(gym.Wrapper):\n","  pass"],"metadata":{"id":"6QNpnZOk_TM4","executionInfo":{"status":"ok","timestamp":1658567942358,"user_tz":-60,"elapsed":14,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class ScaleFrameToFloat(gym.Wrapper):\n","  pass"],"metadata":{"id":"ZQX7xcvk_YYV","executionInfo":{"status":"ok","timestamp":1658567942358,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class LazyFrames(object):\n","  pass"],"metadata":{"id":"dq519GqP_ls1","executionInfo":{"status":"ok","timestamp":1658567942359,"user_tz":-60,"elapsed":14,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class PyTorchImageFrame(gym.ObservationWrapper):\n","  pass"],"metadata":{"id":"7bmlmzTbAN5b","executionInfo":{"status":"ok","timestamp":1658567942359,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# ensure each wrapper is inheriting the right type\n","# Fix all necessary?"],"metadata":{"id":"fgHcd_78AZSe","executionInfo":{"status":"ok","timestamp":1658567942359,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":15,"outputs":[]}]}