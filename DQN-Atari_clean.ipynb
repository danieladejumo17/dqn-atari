{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN-Atari_clean.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNtC7LzGDXJRibNGhkr0lMT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Install Libraries and Dependencies"],"metadata":{"id":"Y3abUQ9LxNAh"}},{"cell_type":"markdown","source":["#### Virtual Display"],"metadata":{"id":"VAXDcw60xxAJ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"gHAUDJCzxCWT","executionInfo":{"status":"ok","timestamp":1658765595932,"user_tz":-60,"elapsed":38581,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"outputs":[],"source":["%%capture\n","!apt install python-opengl # Python binding to OpenGL and related APIs\n","!pip install pyglet==1.5.1 \n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay\n","\n","# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"markdown","source":["#### Gym [Atari]"],"metadata":{"id":"xtIQcKTvx3EB"}},{"cell_type":"code","source":["%%capture\n","!pip install gym[atari,accept-rom-license]==0.21.0"],"metadata":{"id":"BUQY8UwvxnhO","executionInfo":{"status":"ok","timestamp":1658765626417,"user_tz":-60,"elapsed":29997,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"HhFdxZZ0yGbA"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import cv2\n","\n","import gym"],"metadata":{"id":"sqFADgdPyAte","executionInfo":{"status":"ok","timestamp":1658765629982,"user_tz":-60,"elapsed":3575,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Gym Environment"],"metadata":{"id":"rFmt_vWbyKi0"}},{"cell_type":"code","source":["ENV_NAME = \"BreakoutNoFrameskip-v4\"\n","env = gym.make(ENV_NAME)"],"metadata":{"id":"LFbiWOD_yJ7O","executionInfo":{"status":"ok","timestamp":1658765630510,"user_tz":-60,"elapsed":540,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["OBSERVATION_SHAPE = env.observation_space.shape\n","ACTION_SPACE_SIZE = env.action_space.n"],"metadata":{"id":"9BG5wjOjyZH0","executionInfo":{"status":"ok","timestamp":1658765630511,"user_tz":-60,"elapsed":18,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#### Environment Wrappers"],"metadata":{"id":"K-ENUhB_ykjG"}},{"cell_type":"markdown","source":["Wrappers doing the following will be implemented:\n","- Downscalling the observation\n","- Grayscaling the observation\n","- Preprocessing the observation to `n` sequence of past frames\n","- Perform `no-op` operations at the start of an episode\n","- Rescaling the reward to discrete values -1, 0, 1\n","- Perform Frame Skipping\n","- Convert the observations to PyTorch Image shapes and `torch.Tensor` object\n","- Make atari end-of-life the end-of-episode, and reset only on game-over.\n"],"metadata":{"id":"dw9jvg8CywBz"}},{"cell_type":"code","source":["cv2.ocl.setUseOpenCL(False) # function\n","\n","# Ref: Extended Data Table 1 | List of hyperparameters and their values\n","class NoOpOnReset(gym.Wrapper):\n","  def __init__(self, env, noop_max=30, noop_action=0):\n","    \"\"\"Applies `noop_action` for a random number of steps in the range \n","    `[1, noop_max]` at the start of an episode\n","    \"\"\"\n","    super().__init__(env)\n","    \n","    self.noop_max = noop_max\n","    self.noop_action = noop_action\n","\n","    assert env.unwrapped.get_action_meanings()[noop_action] == \"NOOP\", \"Action meaning for noop_action doesn't match 'NOOP'\"\n","    assert noop_max >= 1, \"noop_max must be >= 1\"\n","\n","  def reset(self, **kwargs):\n","    \"\"\"Do no-op action for a random number of steps in [1, noop_max]\"\"\"\n","    num_noop_actions = self.unwrapped.np_random.integers(1, self.noop_max + 1)\n","\n","    obs = self.env.reset(**kwargs)\n","\n","    for _ in range(num_noop_actions):\n","      obs, _, done, _ = self.env.step(self.noop_action)\n","\n","      if done:\n","        obs = self.env.reset(**kwargs)\n","\n","    return obs\n","\n","  def step(self, action):\n","    \"\"\"Perform a single `action` in the environment\"\"\"\n","    return self.env.step(action)  # needed?"],"metadata":{"id":"PyJhRFvoyjll","executionInfo":{"status":"ok","timestamp":1658765630511,"user_tz":-60,"elapsed":14,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class FireOnReset(gym.Wrapper):\n","  def __init__(self, env):\n","    \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n","    super().__init__(env)\n","    \n","    assert env.unwrapped.get_action_meanings()[1] == \"FIRE\", \"Action 1 should be 'FIRE'\"\n","    \n","    # there should be at least one more action apart from NOOP and FIRE\n","    assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","  def reset(self, **kwargs):\n","    self.env.reset(**kwargs)\n","\n","    obs, _, done, info =  self.env.step(1)\n","    if done:\n","      self.env.reset(**kwargs)\n","    \n","    # why take action 2? (this follows from openai baselines implementation)\n","    obs, _, done, info = self.env.step(2)\n","    if done:\n","      self.env.reset(**kwargs)\n","\n","    return obs\n","\n","  def step(self, act):\n","    return self.env.step(act)"],"metadata":{"id":"IZIPjyuamvRD","executionInfo":{"status":"ok","timestamp":1658765630512,"user_tz":-60,"elapsed":14,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class EpisodicLifeEnv(gym.Wrapper):\n","  def __init__(self, env):\n","    \"\"\"Make end-of-life == end-of-episode, but only resets on true game over\"\"\"\n","    super().__init__(env)\n","\n","    # number of lives the agent has left\n","    self.lives = env.unwrapped.ale.lives()\n","    self.real_done = False  # true game over\n","\n","    assert env.unwrapped.get_action_meanings()[0] == \"NOOP\", \"Meaning of action 0 is not NOOP\"\n","\n","  def step(self, act):\n","    obs, rew, done, info = self.env.step(act)\n","    \n","    # if env returns done=True, then it's game over\n","    self.real_done = done\n","\n","    # get how many the agent have left\n","    lives = self.env.unwrapped.ale.lives()\n","\n","    # if there is a reduction in `lives` compared with previous value stored\n","    # in `self.lives`, that's an end-of-life transition\n","    if lives < self.lives and lives > 0:\n","      # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n","      # so its important to keep lives > 0\n","      done = True\n","\n","    # update the number of lives the agent has left\n","    self.lives = lives\n","    return obs, rew, done, info\n","\n","  def reset(self, **kwargs):\n","    \"\"\"Reset only when lives are exhausted.\n","    This way all states are still reachable even though lives are episodic,\n","    and the learner need not know about any of this behind-the-scenes.\n","    \"\"\"\n","    if self.real_done:\n","      obs = self.env.reset(**kwargs)\n","    else:\n","      # take no-op action to advance from lost life state\n","      obs, _, _, _ = self.env.step(0)\n","\n","    # update the number of lives the agent has left\n","    # in-case the no-op action led to end-of-life\n","    self.lives = self.env.unwrapped.ale.lives()\n","    return obs\n","  "],"metadata":{"id":"0zRwFQ440ecx","executionInfo":{"status":"ok","timestamp":1658765630512,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class FrameSkip(gym.Wrapper): # MaxAndSkipEnv\n","  def __init__(self, env, skip=4):\n","    \"\"\"Return only every `skip`-th frame\"\"\"\n","    super().__init__(env)\n","    self.skip = skip\n","    self.obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n","\n","    assert skip >= 1, \"Number of frames to skip should be at least 1\"\n","\n","  def step(self, act):\n","    \"\"\"Repeat action for `skip` frames, sum reward, and max over last observations.\"\"\"\n","    total_reward = 0.0\n","    for i in range(self.skip):\n","      obs, rew, done, info = self.env.step(act)\n","      if i == self.skip - 2: self.obs_buffer[0] = obs\n","      if i == self.skip - 1: self.obs_buffer[1] = obs\n","\n","      total_reward += rew\n","\n","      if done:\n","        # note that the observervation after done does not matter\n","        # so it doesn't matter if we have refilled the obs_buffer or not\n","        break\n","\n","    max_frame = self.obs_buffer.max(axis=0)\n","    return max_frame, total_reward, done, info\n","\n","  def reset(self, **kwargs):\n","    return self.env.reset(**kwargs)\n","\n","  def reset(self):\n","    return self.env.reset()\n"],"metadata":{"id":"pA_Dvvdt-E_c","executionInfo":{"status":"ok","timestamp":1658765630513,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class ClipReward(gym.RewardWrapper):\n","  def __init__(self, env):\n","    super().__init__(env)\n","\n","  def reward(self, rew):\n","    \"\"\"Bin reward to {+1, 0, -1} by its sign.\n","    Return -1 if rew < 0, 0 if rew == 0, +1 if rew > 0\n","    \"\"\"\n","    return np.sign(rew)"],"metadata":{"id":"m3mOBwkf-pFg","executionInfo":{"status":"ok","timestamp":1658765630514,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from gym import spaces\n","\n","class WarpFrame(gym.ObservationWrapper):\n","  def __init__(self, env):\n","    \"\"\"Convert frames to grayscale and downscale to 84x84.\n","    Expects inputs to be of shape height x width x num_channels.\n","    \"\"\"\n","    super().__init__(env)\n","    self.width = 84\n","    self.height = 84\n","    self.observation_space = spaces.Box(low=0, high=255, \n","                                        shape=(self.height, self.width, 1), \n","                                        dtype=np.uint8)\n","\n","  def observation(self, frame):\n","    \"\"\"Convert frame to grayscale and downsscale to 84x84\n","    Return observation with shape 84x84x1\n","    \"\"\"\n","    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n","    frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n","    return frame[:, :, None]"],"metadata":{"id":"f3VkVkBX_EiS","executionInfo":{"status":"ok","timestamp":1658765630514,"user_tz":-60,"elapsed":12,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from collections import deque\n","\n","class StackFrame(gym.Wrapper):  #### TODO: Review\n","  def _init__(self, env, k):\n","    \"\"\"Stack k last frames.\n","    Returns lazy array, which is much more memory efficient.\n","    Expects inputs to be of shape num_channels x height x width.\n","    \"\"\"\n","    super().__init__(env)\n","\n","    self.k = k\n","    self.frames = deque([], maxlen=k)\n","    frame_shape = env.observation_space.shape\n","    self.observation_space = spaces.Box(low=0, high=255, \n","                                        shape=(frame_shape[0]*k, frame_shape[1], frame_shape[2]),\n","                                        dtype=np.uint8)\n","\n","  def step(self, act):\n","    obs, rew, done, info = self.env.step(act)\n","    self.frames.append(obs)\n","    return self.get_obs(), rew, done, info\n","\n","  def reset(self, **kwargs): ###### watch **kwargs\n","    obs = self.env.reset(**kwargs)\n","\n","    for _ in range(self.k):\n","      self.frames.append(obs)\n","    \n","    return self.get_obs()\n","\n","  def get_obs(self):\n","    assert len(self.frames) == self.k, \"Length of frame stack array less than k\"\n","    return LazyFrames(list(self.frames))\n","    "],"metadata":{"id":"6QNpnZOk_TM4","executionInfo":{"status":"ok","timestamp":1658766901270,"user_tz":-60,"elapsed":506,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["class ScaleFrameToFloat(gym.ObservationWrapper):\n","  def __init__(self, env):\n","    super.__init__(env)\n","\n","    self.observation_space = spaces.Box(low=0, high=1, \n","                                        shape=env.observation_space.shape, \n","                                        dtype=np.float32)\n","\n","  def observation(self, obs):\n","    return np.array(obs).astype(np.float32)/255.0"],"metadata":{"id":"ZQX7xcvk_YYV","executionInfo":{"status":"ok","timestamp":1658766893320,"user_tz":-60,"elapsed":529,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class LazyFrames(object):\n","  def __init__(self, frames):\n","    self.frames = frames\n","\n","  def __array__(self, dtype=None):\n","    out = np.concatenate(self.frames, axis=0)\n","    if dtype is not None:\n","      out = out.astype(dtype)\n","    \n","    return out\n","\n","  def __len__(self):\n","    return len(self.frames)\n","\n","  def __getitem__(self, i):\n","    return self.frames[i]\n","    "],"metadata":{"id":"dq519GqP_ls1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PyTorchImageFrame(gym.ObservationWrapper):\n","  def __init__(self, frames):\n","    \"\"\"Image shape to num_channels x height x width\"\"\"\n","    super.__init__(env)\n","    frame_shape = env.observation_space.shape\n","    self.observation_shape = spaces.Box(low=0, high=1, \n","                                        shape=((frame_shape[-1],) + frame_shape[:-1]),\n","                                        dtype=np.float32)\n","\n","  def observation(self, obs):\n","    return np.moveaxis(obs,2,0)\n"],"metadata":{"id":"7bmlmzTbAN5b"},"execution_count":null,"outputs":[]}]}